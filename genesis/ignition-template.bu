variant: fcos
version: 1.5.0
# Fedora CoreOS Ignition Config (Butane format)
# This template is processed by generate-ignition.sh

storage:
  # Format HDDs for Longhorn storage
  # Adjust device paths based on your hardware (check with: lsblk -o NAME,SIZE,TYPE,MODEL)
  filesystems:
    - device: /dev/sdb
      format: ext4
      label: longhorn-disk1
      wipe_filesystem: false
    - device: /dev/sdc
      format: ext4
      label: longhorn-disk2
      wipe_filesystem: false

  # Create mount points
  directories:
    - path: /var/mnt/disk1
      mode: 0755
    - path: /var/mnt/disk2
      mode: 0755

  files:
    # Install Avahi on first boot
    - path: /usr/local/bin/install-avahi.sh
      mode: 0755
      contents:
        inline: |
          #!/bin/bash
          
          # Check if avahi is already installed
          if rpm -q avahi > /dev/null 2>&1; then
            echo "Avahi already installed"
            exit 0
          fi
          
          # Install avahi, avahi-tools, and nss-mdns for .local resolution
          echo "Installing Avahi for mDNS/.local resolution..."
          
          # Try --apply-live first, but don't fail if it doesn't work
          if rpm-ostree install --apply-live --allow-inactive avahi avahi-tools nss-mdns 2>&1 | tee /var/log/avahi-install.log; then
            echo "Avahi installed with --apply-live"
            
            # Try to enable and start (might not work with --apply-live)
            systemctl enable avahi-daemon.service 2>/dev/null || true
            systemctl start avahi-daemon.service 2>/dev/null || true
            
            # Check if avahi-daemon is actually running
            if systemctl is-active avahi-daemon.service >/dev/null 2>&1; then
              echo "Avahi daemon started successfully"
            else
              echo "Avahi installed but daemon not started - will be available after reboot"
            fi
          else
            echo "Failed to install Avahi with --apply-live, trying standard install..."
            rpm-ostree install avahi avahi-tools nss-mdns
            echo "Avahi installed - will be available after reboot"
          fi
          
          exit 0

    # Network configuration for enp2s0 (Management/General traffic VLAN)
    # This interface carries: SSH, k3s API, general cluster traffic
    # Note: Odroid uses enp1s0/enp2s0 interface names, not eth0/eth1
    # DNS: Use Cloudflare/Google directly to avoid circular dependency with Pi-hole
    - path: /etc/NetworkManager/system-connections/enp2s0.nmconnection
      mode: 0600
      contents:
        inline: |
          [connection]
          id=enp2s0-management
          type=ethernet
          interface-name=enp2s0
          autoconnect=true
          autoconnect-priority=100
          
          [ipv4]
          method=auto
          dns=1.1.1.1;8.8.8.8;
          ignore-auto-dns=true
          
          [ipv6]
          method=auto
          addr-gen-mode=stable-privacy

    # Network configuration for enp1s0 (Storage/Longhorn traffic VLAN)
    # This interface carries: Longhorn replication, storage I/O
    - path: /etc/NetworkManager/system-connections/enp1s0.nmconnection
      mode: 0600
      contents:
        inline: |
          [connection]
          id=enp1s0-storage
          type=ethernet
          interface-name=enp1s0
          autoconnect=true
          autoconnect-priority=90
          
          [ipv4]
          method=auto
          may-fail=false
          
          [ipv6]
          method=auto
          addr-gen-mode=stable-privacy

    # Hostname
    - path: /etc/hostname
      mode: 0644
      contents:
        inline: |
          {{NODE_HOSTNAME}}

    # Avahi configuration for .local mDNS resolution
    - path: /etc/avahi/avahi-daemon.conf
      mode: 0644
      contents:
        inline: |
          [server]
          host-name={{NODE_HOSTNAME}}
          domain-name=local
          use-ipv4=yes
          use-ipv6=yes
          allow-interfaces=enp2s0
          deny-interfaces=enp1s0
          ratelimit-interval-usec=1000000
          ratelimit-burst=1000
          
          [wide-area]
          enable-wide-area=yes
          
          [publish]
          publish-addresses=yes
          publish-hinfo=yes
          publish-workstation=yes
          publish-domain=yes
          
          [reflector]
          enable-reflector=no
          
          [rlimits]
          rlimit-core=0
          rlimit-data=4194304
          rlimit-fsize=0
          rlimit-nofile=768
          rlimit-stack=4194304
          rlimit-nproc=3

    # Avahi workstation service for .local discovery
    - path: /etc/avahi/services/workstation.service
      mode: 0644
      contents:
        inline: |
          <?xml version="1.0" standalone='no'?>
          <!DOCTYPE service-group SYSTEM "avahi-service.dtd">
          <service-group>
            <name replace-wildcards="yes">%h</name>
            <service>
              <type>_workstation._tcp</type>
              <port>9</port>
            </service>
          </service-group>

    # k3s installation script
    - path: /usr/local/bin/install-k3s.sh
      mode: 0755
      contents:
        inline: |
          #!/bin/bash
          # Install k3s
          curl -sfL https://get.k3s.io | \
            INSTALL_K3S_VERSION="v1.31.3+k3s1" \
            INSTALL_K3S_EXEC="server \
              --disable=traefik \
              --disable=servicelb \
              --write-kubeconfig-mode=644 \
              --cluster-init" \
            sh -

    # Longhorn prerequisites
    - path: /etc/sysctl.d/99-longhorn.conf
      mode: 0644
      contents:
        inline: |
          # Longhorn requirements
          vm.max_map_count = 262144
          fs.inotify.max_user_instances = 8192
          fs.inotify.max_user_watches = 524288

    # Load iscsi_tcp module for Longhorn
    - path: /etc/modules-load.d/longhorn.conf
      mode: 0644
      contents:
        inline: |
          iscsi_tcp

    # Helper script to get storage network IP (eth1)
    - path: /usr/local/bin/get-storage-ip
      mode: 0755
      contents:
        inline: |
          #!/bin/bash
          # Get the IPv4 address of enp1s0 (storage network)
          ip -4 addr show enp1s0 | grep -oP '(?<=inet\s)\d+(\.\d+){3}' | head -n1

    # Script to publish cluster-wide .local alias
    - path: /usr/local/bin/publish-cluster-alias
      mode: 0755
      contents:
        inline: |
          #!/bin/bash
          # Publish cluster-wide alias so all nodes respond to cluster.local
          # This enables round-robin DNS style access
          CLUSTER_NAME="{{CLUSTER_NAME}}"
          avahi-publish-alias "${CLUSTER_NAME}.local" "{{NODE_HOSTNAME}}.local"

systemd:
  units:
    # Mount Longhorn storage disks
    - name: var-mnt-disk1.mount
      enabled: true
      contents: |
        [Unit]
        Description=Mount Longhorn Disk 1
        Before=local-fs.target

        [Mount]
        What=/dev/disk/by-label/longhorn-disk1
        Where=/var/mnt/disk1
        Type=ext4
        Options=defaults,noatime

        [Install]
        WantedBy=local-fs.target

    - name: var-mnt-disk2.mount
      enabled: true
      contents: |
        [Unit]
        Description=Mount Longhorn Disk 2
        Before=local-fs.target

        [Mount]
        What=/dev/disk/by-label/longhorn-disk2
        Where=/var/mnt/disk2
        Type=ext4
        Options=defaults,noatime

        [Install]
        WantedBy=local-fs.target

    # Install Avahi on first boot (before avahi-daemon starts)
    - name: install-avahi.service
      enabled: true
      contents: |
        [Unit]
        Description=Install Avahi for mDNS resolution
        Wants=network-online.target
        After=network-online.target
        Before=avahi-daemon.service publish-cluster-alias.service
        ConditionPathExists=!/var/lib/avahi-installed
        
        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStart=/usr/local/bin/install-avahi.sh
        ExecStartPost=/usr/bin/touch /var/lib/avahi-installed
        # Don't mark as failed if script returns non-zero
        # The script handles errors internally
        SuccessExitStatus=0 1
        
        [Install]
        WantedBy=multi-user.target

    # Avahi mDNS service for .local address resolution
    - name: avahi-daemon.service
      enabled: true

    # Publish cluster-wide .local alias
    - name: publish-cluster-alias.service
      enabled: true
      contents: |
        [Unit]
        Description=Publish cluster-wide mDNS alias
        After=avahi-daemon.service
        Requires=avahi-daemon.service
        
        [Service]
        Type=simple
        ExecStart=/usr/local/bin/publish-cluster-alias
        Restart=always
        RestartSec=10
        
        [Install]
        WantedBy=multi-user.target

    # k3s service
    - name: k3s.service
      enabled: true
      contents: |
        [Unit]
        Description=Lightweight Kubernetes
        Documentation=https://k3s.io
        Wants=network-online.target avahi-daemon.service
        After=network-online.target avahi-daemon.service

        [Service]
        Type=notify
        EnvironmentFile=-/etc/default/%N
        EnvironmentFile=-/etc/sysconfig/%N
        KillMode=process
        Delegate=yes
        # Having non-zero Limit*s causes performance problems due to accounting overhead
        # in the kernel. We recommend using cgroups to do container-local accounting.
        LimitNOFILE=1048576
        LimitNPROC=infinity
        LimitCORE=infinity
        TasksMax=infinity
        TimeoutStartSec=0
        Restart=always
        RestartSec=5s
        ExecStartPre=/bin/sh -xc '! /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service 2>/dev/null'
        ExecStartPre=-/sbin/modprobe br_netfilter
        ExecStartPre=-/sbin/modprobe overlay
        ExecStartPre=/usr/local/bin/install-k3s.sh
        ExecStart=/usr/local/bin/k3s server
        
        [Install]
        WantedBy=multi-user.target

    # Load kernel modules for k3s
    - name: systemd-modules-load.service
      enabled: true

    # Apply sysctl settings
    - name: systemd-sysctl.service
      enabled: true

    # Auto-update configuration (Zincati)
    - name: zincati.service
      enabled: true

passwd:
  users:
    - name: core
      ssh_authorized_keys:
        - {{SSH_KEY}}
      groups:
        - wheel
        - sudo

